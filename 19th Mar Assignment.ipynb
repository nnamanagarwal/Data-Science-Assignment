{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max scaling, also known as normalization, is a technique used in data preprocessing to transform numerical features to a common scale. It involves rescaling the values of a feature to a range of [0,1].\n",
    "\n",
    "- formula for normalization\n",
    "\n",
    "x(norml) = [ x(i) - x(min) ] / [x(max) - x(min)]\n",
    "\n",
    "x(norml) = normalised value\n",
    "x(i) = original value\n",
    "x(min) = minimum value\n",
    "x(max) = maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.11111111],\n",
       "       [0.22222222],\n",
       "       [0.33333333],\n",
       "       [0.44444444],\n",
       "       [0.55555556],\n",
       "       [0.66666667],\n",
       "       [0.77777778],\n",
       "       [0.88888889],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# creat df\n",
    "x = [1,2,3,4,5,6,7,8,9,10]\n",
    "df = pd.DataFrame(x)\n",
    "\n",
    "# create object\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "# transform the df to normalized \n",
    "min_max.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization, is a feature scaling technique used to scale the features of a dataset so that they have a length of 1. This technique scales each feature independently so that it falls between -1 and 1. This is achieved by dividing each feature value by the length of the feature vector.\n",
    "\n",
    "Unlike Min-Max scaling, which scales the feature values to a fixed range, the Unit Vector technique preserves the direction of the feature vector.\n",
    "This we can understand by this eg. if we have a feature with a range of values between 5 and 25, and we apply Min-Max scaling to scale them between 0 and 1, the feature values will be transformed to the range between 0 and 1, but their direction will change. A value of 5 will become 0, and a value of 25 will become 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078544</td>\n",
       "      <td>0.996911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.054862</td>\n",
       "      <td>0.998494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.637568</td>\n",
       "      <td>0.770394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407574</td>\n",
       "      <td>0.913172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.390874</td>\n",
       "      <td>0.920444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.894427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.522382</td>\n",
       "      <td>0.852712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.383378</td>\n",
       "      <td>0.923592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.256598</td>\n",
       "      <td>0.966518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.084314</td>\n",
       "      <td>0.996439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.078544  0.996911\n",
       "1  0.054862  0.998494\n",
       "2  0.637568  0.770394\n",
       "3  0.407574  0.913172\n",
       "4  0.390874  0.920444\n",
       "5  0.447214  0.894427\n",
       "6  0.522382  0.852712\n",
       "7  0.383378  0.923592\n",
       "8  0.256598  0.966518\n",
       "9  0.084314  0.996439"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe\n",
    "df = pd.DataFrame( {'A':np.random.randint(10,100,10), 'B': np.random.randint(101, 201, 10)})\n",
    "\n",
    "# using normalize module of sklearn normalizing the df\n",
    "data =normalize(df)\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA stands for Principal Component Analysis, which is a technique used for reducing the dimensionality of a dataset while retaining the most important information in it. In other words, PCA is a method that transforms a high-dimensional dataset into a lower-dimensional space while preserving as much of the original information as possible.\n",
    "\n",
    "PCA works by identifying the most significant directions, or principal components, of variation in the dataset. These principal components are then used to create a new set of coordinates that represents the dataset in a lower-dimensional space.\n",
    "\n",
    "__Example:__\n",
    "\n",
    "Suppose we have a dataset that contains information about the height, weight, age, and income of a group of people. This dataset has four dimensions, which makes it difficult to visualize and analyze. We can use PCA to reduce the dimensionality of this dataset by identifying the principal components of variation and projecting the dataset onto these components.\n",
    "\n",
    "After applying PCA, we might find that the first principal component is strongly correlated with height and weight, while the second principal component is strongly correlated with age and income. We can then create a new set of coordinates that represents the dataset in terms of these two principal components, which would enable us to visualize and analyze the dataset more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PCA (Principal Component Analysis)__ can be used for feature extraction as it helps to identify the most important features in a dataset. It achieves this by reducing the dimensionality of the data while retaining the most important information. The retained information can then be used as the extracted features.\n",
    "\n",
    "PCA works by identifying the principal components in the dataset, which are the directions in the data that have the most variance. These principal components are then used to transform the data into a lower-dimensional space.\n",
    "\n",
    "- Here's an example of how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with 1000 features. We want to reduce the dimensionality of the dataset to only 10 features while retaining the most important information. We can use PCA for this task.\n",
    "\n",
    "First, we standardize the data by subtracting the mean and dividing by the standard deviation. Then, we apply PCA to the standardized data to obtain the principal components. We can choose to retain the top 10 principal components as they explain the most variance in the data.\n",
    "\n",
    "Finally, we use the top 10 principal components as the extracted features and use them in our machine learning model. The advantage of using PCA for feature extraction is that it reduces the dimensionality of the data while retaining the most important information, which can improve the performance of the model and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data to ensure that all features are on a similar scale.\n",
    "\n",
    "First, we would identify the features that need to be scaled, such as price, rating, and delivery time. Then, we would apply the Min-Max scaling technique to each of these features independently.\n",
    "\n",
    "Min-Max scaling would transform the range of each feature to lie between 0 and 1, with 0 representing the minimum value of the feature and 1 representing the maximum value of the feature. This transformation ensures that each feature is on the same scale, which is important for many machine learning algorithms.\n",
    "\n",
    "For example, if the price feature ranges from 100 to 1000, and the delivery time feature ranges from 15mins to 120mins, applying Min-Max scaling would transform the price feature to range from 0 to 1, and the delivery time feature to range from 0 to 1. This would allow for easier comparison and analysis of the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>mins</th>\n",
       "      <th>price_n</th>\n",
       "      <th>rating_n</th>\n",
       "      <th>mins_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610</td>\n",
       "      <td>2</td>\n",
       "      <td>111</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>465</td>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.829787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0.503448</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.702128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.563830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.691489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>695</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>0.797241</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.872340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>206</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>0.122759</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.393617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>0.146207</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  rating  mins   price_n  rating_n    mins_n\n",
       "0    610       2   111  0.680000      0.25  1.000000\n",
       "1    465       2    95  0.480000      0.25  0.829787\n",
       "2    482       1    83  0.503448      0.00  0.702128\n",
       "3    422       2    64  0.420690      0.25  0.500000\n",
       "4    198       2    70  0.111724      0.25  0.563830\n",
       "5    842       1    82  1.000000      0.00  0.691489\n",
       "6    117       1    17  0.000000      0.00  0.000000\n",
       "7    695       2    99  0.797241      0.25  0.872340\n",
       "8    206       4    54  0.122759      0.75  0.393617\n",
       "9    223       5    81  0.146207      1.00  0.680851"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# create sample df\n",
    "np.random.seed(123)\n",
    "price = np.random.randint(100, 1000, 10) # assume, price range from 100 rupee to, 1000 rupee\n",
    "rating = np.random.randint(1,6, 10) # assume rating range from 1 star to 5 star\n",
    "mins= np.random.randint(15, 120, 10) # assume time in mins from range 15mins to 120mins\n",
    "\n",
    "df = pd.DataFrame({'price': price, 'rating': rating, 'mins': mins})\n",
    "\n",
    "# importing MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# intiating object\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "# normalize data\n",
    "data = min_max.fit_transform(df[['price', 'rating', 'mins']])\n",
    "\n",
    "# normalize df\n",
    "df_n = pd.DataFrame(data, columns=['price_n', 'rating_n', 'mins_n'])\n",
    "\n",
    "# final output of raw data and normalize data\n",
    "pd.concat([df, df_n], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to reduce the number of features or variables in a dataset while retaining most of the important information. In the context of stock price prediction, the dataset might contain a large number of financial and market trend features, which can make the model complex and difficult to train. Using PCA can help simplify the dataset by identifying the most important features and reducing the dimensionality of the dataset.\n",
    "\n",
    "To use PCA in this scenario, we would first normalize the data using techniques like Min-Max scaling or Standardization. Next, we would apply PCA to the normalized dataset to identify the principal components or the most important features that explain the most variance in the data. These principal components are then used as input features for the stock price prediction model.\n",
    "\n",
    "For example, suppose the dataset contains 20 different features related to company financials and market trends. Applying PCA may identify that only the first 7 principal components are needed to explain 95% of the variance in the data. We could then use these 7 principal components as input features for our stock price prediction model, reducing the dimensionality of the dataset from 20 to 7. This can simplify the model and improve its performance by reducing the chances of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Value\n",
       "0 -1.000000\n",
       "1 -0.578947\n",
       "2 -0.052632\n",
       "3  0.473684\n",
       "4  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# create df\n",
    "data = [1, 5, 10, 15, 20]\n",
    "df = pd.DataFrame(data,columns=[\"values\"])\n",
    "\n",
    "# initalising the object\n",
    "mm_scale = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "# tranforming the data\n",
    "data_n = mm_scale.fit_transform(df)\n",
    "\n",
    "df_n = pd.DataFrame(data_n, columns=['Value'])\n",
    "df_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components to retain using PCA depends on the amount of variance explained by those components. One common approach is to select the smallest number of principal components that can explain at least 80% of the variance in the original dataset.\n",
    "\n",
    "To determine the number of principal components to retain in this specific example, we would first need to calculate the variance explained by each component. This can be done by applying PCA to the dataset and examining the explained variance ratio of each component.\n",
    "\n",
    "If, for example, the first three principal components explain 85% of the variance in the original dataset, we might choose to retain only those three components for feature extraction.\n",
    "\n",
    "It's worth noting that the number of principal components to retain can also depend on the specific goals and requirements of the project.\n",
    "\n",
    "----\n",
    "By : Arjun Verma (GreatArcher101)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
