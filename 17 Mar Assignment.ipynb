{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of a particular value in a variable. There can be various reasons for missing values, such as data entry errors, loss of data during transmission, or a failure to record data for some observation. Handling missing values is essential because they can impact the quality and accuracy of the analysis performed on the dataset.\n",
    "\n",
    "Some of the consequences of missing values are:\n",
    "\n",
    "1. Reduced power and representativeness of the analysis\n",
    "\n",
    "2. Biased results\n",
    "\n",
    "3. Incorrect imputation affect the accuracy of the analysis.\n",
    "\n",
    "some common machine learning algorithms that can handle missing values:\n",
    "\n",
    "1. Decision Trees: Decision trees can handle missing values in the dataset by simply ignoring the missing values and making a decision based on the available data.\n",
    "\n",
    "2. Random Forest: Random forest is an ensemble learning algorithm that uses multiple decision trees to make predictions. It can handle missing values in a similar way to decision trees.\n",
    "\n",
    "3. K-Nearest Neighbors (KNN): KNN can handle missing values by using the available data to find the K-nearest neighbors, and then taking the average of their values to fill in the missing value.\n",
    "\n",
    "4. Naive Bayes: Naive Bayes can handle missing values by ignoring the missing values and making a prediction based on the available data.\n",
    "\n",
    "5. Principal Component Analysis (PCA): PCA is a technique used for dimensionality reduction. It can handle missing values by ignoring the missing values during the computation of the principal components.\n",
    "\n",
    "6. Gradient Boosting: Gradient boosting is another ensemble learning algorithm that can handle missing values by using decision trees to fill in the missing values.\n",
    "\n",
    "7. Neural Networks: Neural networks can handle missing values by using backpropagation to adjust the weights of the network based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some techniques used to handle missing data. With example of each with python code.\n",
    "\n",
    "1. __Deletion:__ In this technique, we remove the missing values from the dataset. There are three types of deletion: listwise deletion, pairwise deletion, and complete case deletion. Listwise deletion removes entire rows with missing values, pairwise deletion removes pairs of values that are missing, and complete case deletion removes cases with any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x     y\n",
       "0  2.0   9.0\n",
       "1  3.0   3.0\n",
       "2  4.0   NaN\n",
       "3  6.0   4.0\n",
       "4  NaN   7.0\n",
       "5  8.0  12.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = [2,3,4,6, None, 8]\n",
    "y = [9,3,None, 4, 7, 12]\n",
    "\n",
    "df = pd.DataFrame({'x':, \"y\":y})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x     y\n",
       "0  2.0   9.0\n",
       "1  3.0   3.0\n",
       "3  6.0   4.0\n",
       "5  8.0  12.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Mean/median imputation:__ In this technique, the missing values are replaced with the mean or median of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.0\n",
       "1    3.0\n",
       "2    4.0\n",
       "3    6.0\n",
       "4    4.6\n",
       "5    8.0\n",
       "Name: x, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['x'].fillna(df['x'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9.0\n",
       "1     3.0\n",
       "2     7.0\n",
       "3     4.0\n",
       "4     7.0\n",
       "5    12.0\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].fillna(df['y'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Mode imputation:__ In this technique, the missing values are replaced with the mode of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x\n",
       "0  a\n",
       "1  b\n",
       "2  c\n",
       "3  a\n",
       "4  a\n",
       "5  b\n",
       "6  a"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'x': ['a', 'b', 'c', 'a', None , 'b', 'a']})\n",
    "\n",
    "df.fillna(df['x'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Interpolation technique:__ In this case, the missing values are replaced with the values obtained by connecting the nearest known values with a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating df\n",
    "x = [1,2,3,4, None, 6]\n",
    "y = [2,4,None, 6, None, 10]\n",
    "\n",
    "df = pd.DataFrame({'A':x, \"B\":y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  1.0   2.0\n",
       "1  2.0   4.0\n",
       "2  3.0   5.0\n",
       "3  4.0   6.0\n",
       "4  5.0   8.0\n",
       "5  6.0  10.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpolating values based on the distribution type as here it is linear\n",
    "df.interpolate(method= 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the number of observations belonging to one class in a classification problem is much higher or much lower than the number of observations belonging to the other classes. In other words, the distribution of class labels in the data is not equal or balanced.\n",
    "\n",
    "If imbalanced data is not handled, model may cause these problems:\n",
    "\n",
    "#### 1.  Bias towards the majority class:\n",
    "\n",
    "    The model may be biased towards the majority class, leading to an over-representation of the majority class in the predictions.\n",
    "\n",
    "#### 2. Poor generalization:\n",
    "\n",
    "    The model may not generalize well to new data if the class distribution in the new data is different from the class distribution in the training data.\n",
    "\n",
    "#### 3. Lower predictive performance:\n",
    "\n",
    "    The model may have lower accuracy, precision, recall, and F1 score for the minority class, resulting in lower predictive performance overall.\n",
    "\n",
    "#### 4. Incorrect ranking:\n",
    "\n",
    "    The model may incorrectly rank the instances in the minority class, leading to a higher false negative rate and missing important instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and DownSampling\n",
    "__Down-sampling:__\n",
    "\n",
    "        In this technique, the majority class is randomly reduced to match the number of observations in the minority class. This can result in a smaller dataset but can help balance the class distribution.\n",
    "\n",
    "__Up-sampling:__\n",
    "\n",
    "        In this technique, the minority class is randomly duplicated to match the number of observations in the majority class. This can result in a larger dataset but can help balance the class distribution.\n",
    "\n",
    "Here are some examples of when up-sampling and down-sampling may be required:\n",
    "\n",
    "__Example of the upsampling:__\n",
    "\n",
    "Up-sampling may be required when the minority class is under-represented, and the model is not correctly identifying instances from that class. For example, in fraud detection, the number of fraudulent transactions may be much lower than non-fraudulent transactions. In this case, up-sampling the minority class can help improve the model's performance.\n",
    "\n",
    "__Down Sampling:__\n",
    "\n",
    "Down-sampling may be required when the majority class is over-represented, and the model is biased towards that class. For example, in medical diagnosis, the number of healthy patients may be much higher than the number of patients with a disease. In this case, down-sampling the majority class can help balance the class distribution and improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Augmentation__\n",
    "\n",
    "Data augmentation is a technique used to increase the size of the training dataset by applying various transformations to the existing data. It is commonly used in machine learning and deep learning to improve the performance of models by increasing the amount and diversity of data available for training. Data augmentation can be applied to various types of data, such as images, text, and time-series data. The most common data augmentation techniques include flipping, cropping, rotation, scaling, and adding noise\n",
    "\n",
    "__SMOTE__\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to handle imbalanced data. It creates synthetic samples of the minority class by interpolating new examples between existing minority class samples. The SMOTE algorithm works by selecting one minority class observation at random and then finding its k nearest minority class neighbors. Synthetic examples are then generated by taking a linear combination of the minority class observation and its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers in a dataset are data points that are significantly different from other observations in the dataset. They can be identified by their extreme values that are either too high or too low compared to the rest of the data. Outliers can occur due to various reasons such as data entry errors, measurement errors, or extreme events, and can have a significant impact on the statistical analysis of the dataset.\n",
    "\n",
    "Handling outliers is necessay for the following reasons:\n",
    "\n",
    "1. Outliers can significantly affect the mean and standard deviation of the dataset, making them unreliable as measures of central tendency and variability. Removing or adjusting outliers can help improve the accuracy of these measures.\n",
    "\n",
    "2. Outliers can affect the distribution of the data, making it non-normal. Many statistical models assume normality, and outliers can violate this assumption.\n",
    "\n",
    "3. Removing or adjusting outliers can help improve the validity of the statistical models.\n",
    "\n",
    "4. Outliers can affect the correlation between variables in the dataset, making it difficult to interpret the relationship between them. Removing or adjusting outliers can help improve the accuracy of correlation analysis.\n",
    "\n",
    "5. Outliers can have a significant impact on the predictive performance of machine learning models. Many machine learning algorithms are sensitive to outliers, and removing or adjusting them can help improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Deletion:__ \n",
    "\n",
    "        In this technique, we remove the missing values from the dataset. There are three types of deletion: listwise deletion, pairwise deletion, and complete case deletion. Listwise deletion removes entire rows with missing values, pairwise deletion removes pairs of values that are missing, and complete case deletion removes cases with any missing value.\n",
    "\n",
    "2. __Mean/Median/Mode imputation:__ \n",
    "\n",
    "        In this method, missing values are replaced with the mean/median/mode of the respective column. This is a simple and quick method but can introduce bias if the missing values are not missing at random.\n",
    "\n",
    "3. __Interpolation technique:__\n",
    "\n",
    "        In this case, the missing values are replaced with the values obtained by connecting the nearest known values with a straight line.\n",
    "\n",
    "4. __Expectation-Maximization (EM):__\n",
    "\n",
    "        EM is an iterative method that estimates the missing data values and model parameters simultaneously. It is commonly used in data analysis, especially in cases where the data is missing not at random.\n",
    "\n",
    "5. __K-Nearest Neighbor (KNN):__\n",
    "\n",
    "        In this technique, missing values are imputed by identifying the K nearest neighbors to the observation with missing data and using the average value of the nearest neighbors to fill in the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data.\n",
    "\n",
    "Some of the commonly used strategies are:\n",
    "\n",
    "- Visualizations:\n",
    "\n",
    "        Visualizations can be used to identify patterns in the missing data. Missing data can be represented graphically, such as through heat maps, histograms, and scatter plots, to identify whether missing data is clustered in particular regions or distributed randomly.\n",
    "\n",
    "- Statistical tests:\n",
    "\n",
    "        Statistical tests can be used to determine whether the missing data is missing at random or is systematically related to other variables in the dataset. Tests such as Little's MCAR test and the chi-square test can be used to determine whether the missing data is missing completely at random or not.\n",
    "\n",
    "- Imputation methods:\n",
    "\n",
    "        Imputation methods can be used to examine the relationship between missing data and other variables in the dataset. For example, imputing missing data using regression imputation can help identify whether the missing data is related to other variables in the dataset.\n",
    "\n",
    "- Machine Learning:\n",
    "\n",
    "        Machine learning algorithms can be used to identify patterns in the missing data. Techniques such as clustering and association rule mining can be used to identify patterns and relationships in the missing data.\n",
    "\n",
    "- Domain knowledge: \n",
    "\n",
    "        In some cases, the domain knowledge of the data can be used to identify patterns in missing data. For example, if the missing data is related to a specific demographic group, it may be possible to infer the missing values based on other variables that are known to be related to that demographic group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with an imbalanced dataset, where one class has significantly fewer samples than the other class, the performance of a machine learning model can be biased towards the majority class. Therefore, it is essential to evaluate the performance of the model using appropriate metrics that consider the class imbalance.\n",
    "\n",
    "Here are some strategies that can be used to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "- __Cost-Sensitive Learning:__\n",
    "\n",
    "Cost-sensitive learning is a technique that assigns different costs to different types of classification errors based on the class imbalance. This approach can help to train a model that is more sensitive to the minority class and reduces the false-negative rate.\n",
    "\n",
    "- __Sampling Techniques:__\n",
    "\n",
    "Sampling techniques like over-sampling and under-sampling can be used to balance the dataset by either increasing the minority class samples or decreasing the majority class samples. This can help to improve the model's performance on the minority class.\n",
    "\n",
    "- __Confusion Matrix:__\n",
    "\n",
    "A confusion matrix can be used to evaluate the performance of the model. It provides information about the number of true positive, false positive, true negative, and false negative predictions. Using this, we can calculate performance metrics like precision, recall, F1 score, and accuracy.\n",
    "\n",
    "- __Adjust class weights:__\n",
    "\n",
    "Some machine learning algorithms such as logistic regression and decision trees allow for adjusting the weights of the different classes. By increasing the weight of the minority class, the algorithm can be forced to focus more on correctly classifying examples from the minority class.\n",
    "\n",
    "- __Collect more data:__\n",
    "\n",
    "Collecting more data from the minority class can help address the class imbalance and improve the performance of the machine learning model. However, this is not always feasible, particularly in medical diagnosis projects where data collection can be expensive and time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset where the majority class overwhelms the minority class, one can use several methods for down-sampling the majority class. Here are some common techniques:\n",
    "\n",
    "- Random under-sampling: This method involves randomly removing samples from the majority class until the class distribution is balanced with the minority class.\n",
    "\n",
    "- Cluster-based under-sampling: This method involves identifying clusters of the majority class samples and randomly removing samples from each cluster until the class distribution is balanced with the minority class.\n",
    "\n",
    "- Tomek links: This method identifies pairs of samples from different classes that are the closest to each other and removes the majority class sample from each pair.\n",
    "\n",
    "- Synthetic Minority Over-sampling Technique (SMOTE-ENN):  To use SMOTE with Edited Nearest Neighbors for down-sampling the majority class, you can first apply SMOTE to up-sample the minority class to a desired level. Then, apply ENN to remove the noisy or irrelevant data points. Finally, you can randomly select a subset of the majority class samples to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset where the minority class has a low percentage of occurrences, we can use various techniques to balance the dataset and up-sample the minority class.\n",
    "\n",
    "Here are some methods that can be used to up-sample the minority class:\n",
    "\n",
    "- Random Over-Sampling:\n",
    "\n",
    "In random over-sampling, we randomly duplicate samples from the minority class until we have a balanced dataset. This method can be quick and easy, but it may result in overfitting and the generation of redundant data.\n",
    "\n",
    "- SMOTE:\n",
    "\n",
    " SMOTE is a popular method for balancing imbalanced datasets. In SMOTE, we create synthetic minority class samples by interpolating between the minority class samples. This can help to increase the size of the minority class and balance the dataset.\n",
    "\n",
    "- ADASYN:\n",
    "\n",
    "ADASYN is a variation of SMOTE that creates synthetic minority class samples based on the density distribution of the dataset. This can help to create more diverse synthetic samples and improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
